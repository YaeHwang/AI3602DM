{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016754,
     "end_time": "2021-12-07T06:01:59.224530",
     "exception": false,
     "start_time": "2021-12-07T06:01:59.207776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Coursework 3 Streaming Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017139,
     "end_time": "2021-12-07T06:01:59.258384",
     "exception": false,
     "start_time": "2021-12-07T06:01:59.241245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Task1ï¼šDGIM**\n",
    "\n",
    "DGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the stream_data_dgim.txt (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019833,
     "end_time": "2021-12-07T06:01:59.296854",
     "exception": false,
     "start_time": "2021-12-07T06:01:59.277021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. Set the window size to 1000, and count the number of 1-bits in the current window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T06:01:59.338917Z",
     "iopub.status.busy": "2021-12-07T06:01:59.337144Z",
     "iopub.status.idle": "2021-12-07T06:01:59.347546Z",
     "shell.execute_reply": "2021-12-07T06:01:59.348122Z"
    },
    "papermill": {
     "duration": 0.032439,
     "end_time": "2021-12-07T06:01:59.348503",
     "exception": false,
     "start_time": "2021-12-07T06:01:59.316064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of 1-bits in the current window: 508\n",
      "DGIM query time: 0.17 ms\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "filename = './stream_data_dgim.txt'\n",
    "\n",
    "class Bucket():\n",
    "    def __init__(self, timestamp, size):\n",
    "        self.timestamp = timestamp\n",
    "        self.size = size\n",
    "\n",
    "class DGIM():\n",
    "    def __init__(self, window_size):\n",
    "        self.window_size = window_size\n",
    "        self.timestamp = 0\n",
    "        self.buckets = deque()\n",
    "    \n",
    "    def update(self, bit):\n",
    "        self.timestamp += 1\n",
    "        \n",
    "        if self.buckets and self.buckets[0].timestamp <= (self.timestamp - self.window_size):\n",
    "            self.buckets.popleft()\n",
    "\n",
    "        if bit == 0:\n",
    "            return\n",
    "        \n",
    "        self.buckets.append(Bucket(self.timestamp, 1))\n",
    "        self.merge()\n",
    "\n",
    "    def index(self):\n",
    "        if len(self.buckets) < 3:\n",
    "            return\n",
    "\n",
    "        for i in range(len(self.buckets) - 1, 1, -1):\n",
    "            curr_size = self.buckets[i].size\n",
    "            if (curr_size == self.buckets[i - 1].size\n",
    "                    and curr_size == self.buckets[i - 2].size):\n",
    "                return i\n",
    "\n",
    "    def merge(self):\n",
    "        i = self.index()\n",
    "\n",
    "        while i is not None:\n",
    "            self.buckets[i - 1].size += self.buckets[i - 2].size\n",
    "            del self.buckets[i - 2]\n",
    "\n",
    "            i = self.index()\n",
    "\n",
    "    def count(self, k):\n",
    "        total_size = 0\n",
    "        last_bucket_size = 0\n",
    "        for i in range(len(self.buckets) - 1, -1, -1):\n",
    "            if self.buckets[i].timestamp <= self.timestamp - k:\n",
    "                break\n",
    "\n",
    "            total_size += self.buckets[i].size\n",
    "            last_bucket_size = self.buckets[i].size\n",
    "\n",
    "        return total_size - int(math.ceil(last_bucket_size / 2.0))\n",
    "\n",
    "def tic():\n",
    "    global _start_time\n",
    "    _start_time = time.perf_counter()\n",
    "\n",
    "\n",
    "def toc():\n",
    "    return 1000 * (time.perf_counter() - _start_time)\n",
    "\n",
    "with open (filename, 'r') as f:\n",
    "    stream = list(map(int, f.read().split()))\n",
    "\n",
    "dgim = DGIM(1000)\n",
    "\n",
    "for bit in stream:\n",
    "    dgim.update(bit)\n",
    "\n",
    "tic()\n",
    "\n",
    "print(\"The number of 1-bits in the current window:\", dgim.count(1000))\n",
    "print(\"DGIM query time: %.2f ms\" % toc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015395,
     "end_time": "2021-12-07T06:01:59.380081",
     "exception": false,
     "start_time": "2021-12-07T06:01:59.364686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. With the window size 1000, count the number of 1-bits in the last 500 and 200 bits of the bitstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T06:01:59.417980Z",
     "iopub.status.busy": "2021-12-07T06:01:59.417325Z",
     "iopub.status.idle": "2021-12-07T06:01:59.420744Z",
     "shell.execute_reply": "2021-12-07T06:01:59.421295Z"
    },
    "papermill": {
     "duration": 0.02556,
     "end_time": "2021-12-07T06:01:59.421483",
     "exception": false,
     "start_time": "2021-12-07T06:01:59.395923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of 1-bits in the last 500 bits of the bitstream: 220\n",
      "The number of 1-bits in the last 200 bits of the bitstream: 76\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of 1-bits in the last 500 bits of the bitstream:\", dgim.count(500))\n",
    "\n",
    "print(\"The number of 1-bits in the last 200 bits of the bitstream:\", dgim.count(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01553,
     "end_time": "2021-12-07T06:01:59.452974",
     "exception": false,
     "start_time": "2021-12-07T06:01:59.437444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. Write a function that accurately counts the number of 1-bits in the current window. Caculate the accuracy of your own DGIM algorithm and compare the running time difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T06:01:59.487896Z",
     "iopub.status.busy": "2021-12-07T06:01:59.487251Z",
     "iopub.status.idle": "2021-12-07T06:01:59.490402Z",
     "shell.execute_reply": "2021-12-07T06:01:59.490914Z"
    },
    "papermill": {
     "duration": 0.022321,
     "end_time": "2021-12-07T06:01:59.491102",
     "exception": false,
     "start_time": "2021-12-07T06:01:59.468781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accurate number of 1-bits in the current window: 391\n",
      "Accurate counter time: 0.44 ms\n",
      "Rel. err. 29.92%\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "\n",
    "print(\"The accurate number of 1-bits in the current window:\", abs(sum(stream[-1000:])))\n",
    "print(\"Accurate counter time: %.2f ms\" % toc())\n",
    "print(\"Rel. err. %.2f%%\" % (100 * abs(sum(stream[-1000:]) - dgim.count(1000)) / sum(stream[-1000:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015507,
     "end_time": "2021-12-07T06:01:59.522321",
     "exception": false,
     "start_time": "2021-12-07T06:01:59.506814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Task2: Bloom Filter**\n",
    "\n",
    "A Bloom filter is a space-efficient probabilistic data structure. Here the task is to implement a bloom filter by yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016544,
     "end_time": "2021-12-07T06:01:59.557915",
     "exception": false,
     "start_time": "2021-12-07T06:01:59.541371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data loading:\n",
    "\n",
    "From the NLTK (Natural Language ToolKit) library, we import a large list of English dictionary words, commonly used by the very first spell-checking programs in Unix-like operating systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T06:01:59.599213Z",
     "iopub.status.busy": "2021-12-07T06:01:59.598428Z",
     "iopub.status.idle": "2021-12-07T06:02:01.538667Z",
     "shell.execute_reply": "2021-12-07T06:02:01.539256Z"
    },
    "papermill": {
     "duration": 1.962095,
     "end_time": "2021-12-07T06:02:01.539496",
     "exception": false,
     "start_time": "2021-12-07T06:01:59.577401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\xxhyy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "nltk.download('words')\n",
    "word_list = words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015919,
     "end_time": "2021-12-07T06:02:01.572063",
     "exception": false,
     "start_time": "2021-12-07T06:02:01.556144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Then we load another dataset from the NLTK Corpora collection: movie_reviews.\n",
    "\n",
    "The movie reviews are categorized between positive and negative, so we construct a list of words (usually called bag of words) for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T06:02:01.608648Z",
     "iopub.status.busy": "2021-12-07T06:02:01.607587Z",
     "iopub.status.idle": "2021-12-07T06:02:06.305611Z",
     "shell.execute_reply": "2021-12-07T06:02:06.304789Z"
    },
    "papermill": {
     "duration": 4.717611,
     "end_time": "2021-12-07T06:02:06.305799",
     "exception": false,
     "start_time": "2021-12-07T06:02:01.588188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\xxhyy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "neg_reviews = []\n",
    "pos_reviews = []\n",
    "\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    neg_reviews.extend(movie_reviews.words(fileid))\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    pos_reviews.extend(movie_reviews.words(fileid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016173,
     "end_time": "2021-12-07T06:02:06.338727",
     "exception": false,
     "start_time": "2021-12-07T06:02:06.322554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here we get a data stream (word_list) and 2 query lists (neg_reviews and pos_reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016981,
     "end_time": "2021-12-07T06:02:06.372258",
     "exception": false,
     "start_time": "2021-12-07T06:02:06.355277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. Write a function that accurately determines whether each word in neg_reviews and pos_reviews belongs to word_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T06:02:06.410004Z",
     "iopub.status.busy": "2021-12-07T06:02:06.408987Z",
     "iopub.status.idle": "2021-12-07T06:02:06.411435Z",
     "shell.execute_reply": "2021-12-07T06:02:06.411920Z"
    },
    "papermill": {
     "duration": 0.02373,
     "end_time": "2021-12-07T06:02:06.412105",
     "exception": false,
     "start_time": "2021-12-07T06:02:06.388375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postive word number: 832564\n",
      "Negative word number: 751256\n"
     ]
    }
   ],
   "source": [
    "def determine(needles, haystack):\n",
    "    hashed = set(needles)\n",
    "    return list(map(hashed.__contains__, haystack))\n",
    "\n",
    "word_pos = determine(word_list, pos_reviews)\n",
    "word_neg = determine(word_list, neg_reviews)\n",
    "\n",
    "print(\"Postive word number:\", len(word_pos))\n",
    "print(\"Negative word number:\", len(word_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016948,
     "end_time": "2021-12-07T06:02:06.445306",
     "exception": false,
     "start_time": "2021-12-07T06:02:06.428358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " ### 2. Implement the bloom filter by yourself and add all words in word_list in your bloom filter. Compare the running time difference between linear search on a list and multiple hash computations in a Bloom filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T06:02:06.481404Z",
     "iopub.status.busy": "2021-12-07T06:02:06.480412Z",
     "iopub.status.idle": "2021-12-07T06:02:06.483982Z",
     "shell.execute_reply": "2021-12-07T06:02:06.484437Z"
    },
    "papermill": {
     "duration": 0.023186,
     "end_time": "2021-12-07T06:02:06.484608",
     "exception": false,
     "start_time": "2021-12-07T06:02:06.461422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan: 328.67 ms\n",
      "Bloom: 1.29 ms\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import hashlib\n",
    "import random\n",
    "random.seed(9527)\n",
    "\n",
    "\n",
    "str = \"QWERTYUIOP[]ASDFGHJKL;ZXCVBNM,./123456789\"\n",
    "\n",
    "\n",
    "class Bloom(object):\n",
    "    def __init__(self, needles, m, k):\n",
    "        super().__init__()\n",
    "        salts = []\n",
    "        self.k = k\n",
    "        self.m = m\n",
    "        for i in range(k):\n",
    "            q = list(str)\n",
    "            random.shuffle(q)\n",
    "            salts.append(''.join(q[:16]))\n",
    "        self.salts = salts\n",
    "        self.hashalg = hashlib.sha1\n",
    "        khash = numpy.zeros([m], dtype=numpy.bool_)\n",
    "        for needle in needles:\n",
    "            for i in range(k):\n",
    "                khash[self.myhash(salts[i], needle, m)] = True\n",
    "        self.khash = khash\n",
    "\n",
    "    def myhash(self, salt, needle, modp):\n",
    "        return int.from_bytes(self.hashalg((salt + needle).encode()).digest(), 'little') % modp\n",
    "\n",
    "    def query(self, haystack_single):\n",
    "        for i in range(self.k):\n",
    "            if not self.khash[self.myhash(self.salts[i], haystack_single, self.m)]:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "bloom = Bloom(word_list, 1000, 4)\n",
    "tic()\n",
    "for i in range(200):\n",
    "    pos_reviews[i] in word_list\n",
    "print(\"Scan: %.2f ms\" % toc())\n",
    "tic()\n",
    "for i in range(200):\n",
    "    bloom.query(pos_reviews[i])\n",
    "print(\"Bloom: %.2f ms\" % toc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016671,
     "end_time": "2021-12-07T06:02:06.518034",
     "exception": false,
     "start_time": "2021-12-07T06:02:06.501363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. Use different bit array length â€˜mâ€™ and number of hash functions â€˜kâ€™ to implement the bloom filter algorithm. Then compare the impact of different m and k on the false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T06:02:06.555617Z",
     "iopub.status.busy": "2021-12-07T06:02:06.554723Z",
     "iopub.status.idle": "2021-12-07T06:02:06.557735Z",
     "shell.execute_reply": "2021-12-07T06:02:06.557108Z"
    },
    "papermill": {
     "duration": 0.023551,
     "end_time": "2021-12-07T06:02:06.557871",
     "exception": false,
     "start_time": "2021-12-07T06:02:06.534320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = 100000 k = 2 fp = 98.57%\n",
      "m = 1000000 k = 2 fp = 46.19%\n",
      "m = 10000000 k = 2 fp = 0.11%\n",
      "m = 1000000 k = 1 fp = 26.60%\n",
      "m = 1000000 k = 2 fp = 8.52%\n",
      "m = 1000000 k = 3 fp = 7.24%\n",
      "m = 1000000 k = 4 fp = 20.03%\n",
      "m = 1000000 k = 6 fp = 16.00%\n",
      "m = 1000000 k = 9 fp = 57.07%\n"
     ]
    }
   ],
   "source": [
    "def bloom_test():\n",
    "    k = 2\n",
    "\n",
    "    for lg10m in range(5, 8):\n",
    "        bloom = Bloom(word_list, 10 ** lg10m, k)\n",
    "        fp = 0\n",
    "        neg = 0\n",
    "\n",
    "        for i, rev in enumerate(pos_reviews):\n",
    "            fp += (not word_pos[i]) and bloom.query(rev)\n",
    "            neg += not word_pos[i]\n",
    "        for i, rev in enumerate(neg_reviews):\n",
    "            fp += (not word_neg[i]) and bloom.query(rev)\n",
    "            neg += not word_neg[i]\n",
    "\n",
    "        print(\"m =\", bloom.m, \"k =\", k, \"fp = %.2f%%\" % (100 * fp / neg))\n",
    "\n",
    "    m = 10 ** 6\n",
    "\n",
    "    for k in [1, 2, 3, 4, 6, 9]:\n",
    "        bloom = Bloom(word_list, m, k)\n",
    "        fp = 0\n",
    "        neg = 0\n",
    "\n",
    "        for i, rev in enumerate(pos_reviews):\n",
    "            fp += (not word_pos[i]) and bloom.query(rev)\n",
    "            neg += not word_pos[i]\n",
    "        for i, rev in enumerate(neg_reviews):\n",
    "            fp += (not word_neg[i]) and bloom.query(rev)\n",
    "            neg += not word_neg[i]\n",
    "            \n",
    "        print(\"m =\", bloom.m, \"k =\", k, \"fp = %.2f%%\" % (100 * fp / neg))\n",
    "\n",
    "bloom_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016085,
     "end_time": "2021-12-07T06:02:06.590608",
     "exception": false,
     "start_time": "2021-12-07T06:02:06.574523",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Task3: Statistics Estimation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016173,
     "end_time": "2021-12-07T06:02:06.623171",
     "exception": false,
     "start_time": "2021-12-07T06:02:06.606998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here we use the query stream (neg_reviews) from task 2 to estimate 1) the number of distinct words appeared, and 2)the surprise number of the stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016175,
     "end_time": "2021-12-07T06:02:06.688114",
     "exception": false,
     "start_time": "2021-12-07T06:02:06.671939",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. \tWrite a function that accurately counts the occurrence times of each word in neg_reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T06:02:06.724213Z",
     "iopub.status.busy": "2021-12-07T06:02:06.723528Z",
     "iopub.status.idle": "2021-12-07T06:02:06.727643Z",
     "shell.execute_reply": "2021-12-07T06:02:06.728104Z"
    },
    "papermill": {
     "duration": 0.023883,
     "end_time": "2021-12-07T06:02:06.728280",
     "exception": false,
     "start_time": "2021-12-07T06:02:06.704397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The occurrence times of each word in neg_reviews: 28480\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "occurrence_times = Counter(neg_reviews)\n",
    "print(\"The occurrence times of each word in neg_reviews:\", len(occurrence_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016192,
     "end_time": "2021-12-07T06:02:06.761062",
     "exception": false,
     "start_time": "2021-12-07T06:02:06.744870",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. Implement the Flajolet-Martin alg. to estimate the number of distinct words occurred. Try multiple hash functions to improve the estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T06:02:06.799077Z",
     "iopub.status.busy": "2021-12-07T06:02:06.798374Z",
     "iopub.status.idle": "2021-12-07T06:02:06.800943Z",
     "shell.execute_reply": "2021-12-07T06:02:06.801435Z"
    },
    "papermill": {
     "duration": 0.024036,
     "end_time": "2021-12-07T06:02:06.801613",
     "exception": false,
     "start_time": "2021-12-07T06:02:06.777577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_hash = 2 est = 1052672.0\n",
      "n_hash = 4 est = 73728.0\n",
      "n_hash = 6 est = 36864.0\n",
      "n_hash = 8 est = 36864.0\n"
     ]
    }
   ],
   "source": [
    "def flajolet_martin():\n",
    "    hasher = Bloom([], 1, 40)\n",
    "    modp = 2 ** 30\n",
    "    est = []\n",
    "\n",
    "    for i in range(8):\n",
    "        r = 0\n",
    "        salt = hasher.salts[i]\n",
    "        hash_fn = hasher.myhash\n",
    "\n",
    "        for word in neg_reviews:\n",
    "            mh = hash_fn(salt, word, modp)\n",
    "            r = max(r, len(bin(mh)) - len(bin(mh).rstrip('0')))\n",
    "        est.append(r)\n",
    "        des = []\n",
    "        for k in range(5):\n",
    "            if len(est[k::5]):\n",
    "                des.append(numpy.mean(2 ** numpy.array(est[k::5])))\n",
    "                \n",
    "        if i % 2 == 1:\n",
    "            print(\"n_hash =\", i + 1, \"est =\", numpy.median(des))\n",
    "\n",
    "flajolet_martin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Estimate the surpise number with limited memory to store words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: 5896116000.0\n",
      "Estimation of surprise number with 1000 : 5760093859.96\n",
      "Estimation of surprise number with 3000 : 5998618641.634666\n",
      "Estimation of surprise number with 10000 : 5917395973.148\n",
      "Estimation of surprise number with 100000 : 5905360423.81208\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def dp(memory_limit=100):\n",
    "    n = len(neg_reviews)\n",
    "    dp = numpy.zeros(n, dtype=numpy.int64)\n",
    "    last_map = dict()\n",
    "\n",
    "    for i in reversed(range(n)):\n",
    "        dp[i] = 1\n",
    "        if neg_reviews[i] in last_map:\n",
    "            dp[i] += dp[last_map[neg_reviews[i]]]\n",
    "        last_map[neg_reviews[i]] = i\n",
    "\n",
    "    return (dp[numpy.random.randint(n, size=[memory_limit])].mean() * 2 - 1) * n\n",
    "\n",
    "\n",
    "def surprise_number(memory_limit=100):\n",
    "    c = 0\n",
    "    n = len(neg_reviews)\n",
    "    track = defaultdict(int)\n",
    "    ts = sorted(numpy.random.randint(n, size=[memory_limit]))[::-1]\n",
    "\n",
    "    for t, w in enumerate(neg_reviews):\n",
    "        while len(ts) and t == ts[-1]:\n",
    "            track[w] += 1\n",
    "            ts.pop()\n",
    "        if w in track:\n",
    "            c += track[w]\n",
    "\n",
    "    return (2 * c - 1) * n / memory_limit\n",
    "\n",
    "\n",
    "print(\"Ground truth:\", numpy.sum(numpy.array(list(occurrence_times.values())).astype(numpy.float32) ** 2))\n",
    "\n",
    "for lim in [1000, 3000, 10000, 100000]:\n",
    "    print(\"Estimation of surprise number with\", lim, \":\", surprise_number(lim))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.419535,
   "end_time": "2021-12-07T06:02:07.730264",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-07T06:01:52.310729",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
